{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0336ff5ee2c1e0e67b5ec251f9bb85714e66dde2cea2c3ccd73e788234e60283a",
   "display_name": "Python 3.7.6 64-bit ('artha@3.7.6')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import inspect\n",
    "import tqdm\n",
    "import contractions\n",
    "from collections import Counter\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "os.sys.path.insert(0,parentdir) \n",
    "\n",
    "import Artha\n",
    "from Artha.nlp_extraction import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "user = \"Nostranomist\"\n",
    "# user = \"CryptoKaleo\"\n",
    "with open(\"../data/tweets/u\"+user+\"tweets.json\", \"r\") as r:\n",
    "    tweets = json.load(r)\n",
    "tweet_text = []\n",
    "for ind, tweet in enumerate(tweets):\n",
    "    sent = tweet[\"full_text\"]\n",
    "    \n",
    "    try: #remove tweets of only tagging someone\n",
    "        while sent[0] == \"@\":\n",
    "            sent = sent.split(\" \", 1)[1]\n",
    "        sent = contractions.fix(sent.replace(\"&amp;\", \"and\").replace(\"@\", \"\"))\n",
    "        tweet_text.append((sent, {\"created_at\":tweet[\"created_at\"], \"id\" :tweet[\"id\"]}))\n",
    "    except:\n",
    "        tweets.pop(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "docs = []\n",
    "all_tickers = []\n",
    "# need to disable pipes\n",
    "with open(\"senti.txt\", \"w+\") as w:\n",
    "    for doc, context in nlp.pipe(tweet_text, as_tuples = True, n_process=-1):\n",
    "        \n",
    "        if doc._.tickers:\n",
    "            all_tickers.extend(doc._.tickers)\n",
    "\n",
    "            w.write(\"***\"+str(ind)+\" \"+\" \".join(doc._.tickers)+\"***\"+str(doc._.polarity)+\" \"+str(doc._.subjectivity)+\"\\n\")\n",
    "            w.write(doc.text+\"\\n\\n\")\n",
    "        doc._.tweet_id = context[\"id\"]\n",
    "        doc._.tweeted_at = context[\"created_at\"]\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]._.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"specific.txt\", \"w+\") as w:\n",
    "    for doc in docs:\n",
    "        if \"ENJ\" in doc._.tickers:\n",
    "            w.write(str(doc._.polarity)+\"\\n\"+doc._.tweeted_at+\"\\n\"+doc.text+\" \"+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(all_tickers).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}